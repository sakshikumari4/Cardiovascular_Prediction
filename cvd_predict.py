"""CVD_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zcYlDtQqOlqYpKT9bEVheQ8uKZvIL8CB
"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('EDA2030').getOrCreate()

# # --- Mount Google Drive ---
from google.colab import drive
drive.mount('/content/drive')

# --- Load Data ---
df = spark.read.csv(r"/content/drive/MyDrive/cleaned_structured_data_final.csv",
                    header=True, inferSchema=True)
df.show(5)
df.printSchema()

# --- EDA: Basic Overview ---
print("Total Rows:", df.count())
print("Columns:", df.columns)

# Count missing values
for col in df.columns:
    print(f"{col}: {df.filter(df[col].isNull()).count()} nulls")

# Summary statistics
df.describe().show()

# --- EDA: Numeric Distributions ---
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql.types import IntegerType, DoubleType, FloatType

numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (IntegerType, DoubleType, FloatType))]


if numeric_cols:
    pdf_numeric = df.select(numeric_cols).toPandas()
    pdf_numeric.hist(bins=20, figsize=(15,10))
    plt.suptitle("Numeric Column Distributions")
    plt.show()
else:
    print("No numeric columns found for plotting histograms.")


# --- EDA: Correlation Matrix ---
if numeric_cols:
    corr_matrix = pdf_numeric.corr()
    import seaborn as sns

    plt.figure(figsize=(10,8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
    plt.title("Correlation Matrix")
    plt.show()
else:
    print("No numeric columns found for plotting correlation matrix.")


# --- EDA: Trends over Years ---
if 'Year' in df.columns and 'Data_Value' in df.columns:
    trend_pdf = df.groupBy('Year').avg('Data_Value').orderBy('Year').toPandas()
    plt.figure(figsize=(12,6))
    plt.plot(trend_pdf['Year'], trend_pdf['avg(Data_Value)'], marker='o')
    plt.title("Average Data_Value Over Years")
    plt.xlabel("Year")
    plt.ylabel("Average Data_Value")
    plt.grid(True)
    plt.show()

# --- EDA: Top and Bottom Locations ---
if 'LocationID' in df.columns and 'Data_Value' in df.columns:
    top_locations = df.groupBy("LocationID").avg("Data_Value").orderBy("avg(Data_Value)", ascending=False).show(10)
    bottom_locations = df.groupBy("LocationID").avg("Data_Value").orderBy("avg(Data_Value)").show(10)

# --- Data Cleaning ---
from pyspark.sql.functions import mean

# Drop rows with missing target
df = df.na.drop(subset=['Data_Value'])

# Fill numeric missing values with mean
for col in numeric_cols:
    mean_val = df.select(mean(col)).first()[0]
    if mean_val is not None:
        df = df.na.fill({col: mean_val})

# Filter obvious outliers
df = df.filter((df['Data_Value'] >= 0) & (df['Data_Value'] < 1000))

# --- Feature Engineering ---
from pyspark.sql.functions import lit
df = df.withColumn('years_to_2030', lit(2030) - df['Year'])

from pyspark.ml.feature import StringIndexer
if 'region' in df.columns:
    indexer = StringIndexer(inputCol='region', outputCol='region_index')
    df = indexer.fit(df).transform(df)

# --- Assemble Features ---
from pyspark.ml.feature import VectorAssembler

feature_cols = ["years_to_2030", "LocationID", "X_long", "Y_lat", "GeographicLevel"]
if 'region_index' in df.columns:
    feature_cols.append('region_index')

assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
data = assembler.transform(df)
data.select(['features', 'Data_Value']).show(5)

# --- Train/Test Split ---
train = data.filter(data["Year"] <= 2018)
test = data.filter((data["Year"] > 2018) & (data["Year"] <= 2020))

# --- Train Linear Regression Model ---
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

lr = LinearRegression(featuresCol="features", labelCol="Data_Value")

paramGrid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

evaluator = RegressionEvaluator(labelCol="Data_Value", predictionCol="prediction", metricName="rmse")

cv = CrossValidator(estimator=lr,
                    estimatorParamMaps=paramGrid,
                    evaluator=evaluator,
                    numFolds=3)

cv_model = cv.fit(train)
best_model = cv_model.bestModel

predictions = best_model.transform(test)
rmse = evaluator.evaluate(predictions)
print(f"Test RMSE: {rmse}")
print(f"Coefficients: {best_model.coefficients}")
print(f"Intercept: {best_model.intercept}")

# --- Predict 2030 ---
df_2030 = df.withColumn("Year", lit(2030))
df_2030 = df_2030.withColumn("years_to_2030", lit(2030) - df_2030["Year"])
data_2030 = assembler.transform(df_2030)
pred_2030 = best_model.transform(data_2030)
pred_2030.select("LocationID", "Year", "prediction").show(10)

# --- Save Model ---
best_model.save("/content/best_linear_model_2030")

# --- Plotting ---
import plotly.express as px

pdf = pred_2030.select("LocationID", "Year", "prediction").toPandas()

plt.figure(figsize=(12,6))
plt.scatter(pdf['LocationID'], pdf['prediction'], c='blue', alpha=0.6)
plt.title('Predicted Death Rate for 2030 by LocationID')
plt.xlabel('LocationID')
plt.ylabel('Predicted Death Rate')
plt.grid(True)
plt.show()

fig = px.scatter(pdf, x='LocationID', y='prediction',
                 color='prediction', size='prediction',
                 title='Predicted Death Rate for 2030 by LocationID',
                 labels={'prediction': 'Predicted Death Rate', 'LocationID': 'Location ID'},
                 hover_data=['Year'], template='plotly_white')
fig.show()

# --- Optional Geo Scatter Map ---
if 'X_long' in df.columns and 'Y_lat' in df.columns:
    geo_pdf = df.select("X_long", "Y_lat", "Data_Value").sample(fraction=0.1, seed=42).toPandas()
    fig_geo = px.scatter_geo(geo_pdf,
                             lon="X_long", lat="Y_lat",
                             color="Data_Value", size="Data_Value",
                             projection="natural earth",
                             title="Geographical Distribution of Death Rates")
    fig_geo.show()

import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import Correlation
import pandas as pd

# Select only numeric columns
numeric_cols = [col for col, dtype in df.dtypes if dtype in ['int', 'double', 'float']]
assembler = VectorAssembler(inputCols=numeric_cols, outputCol="features")
vector_col = assembler.transform(df).select("features")

# Compute the correlation matrix
matrix = Correlation.corr(vector_col, "features").head()
corr_matrix = matrix[0].toArray()
# Convert to pandas DataFrame for plotting
corr_df = pd.DataFrame(corr_matrix, columns=numeric_cols, index=numeric_cols)
# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap', fontsize=14)
plt.show()

import plotly.graph_objects as go
import pandas as pd

# Example DataFrame (replace with your actual data)
data = pd.DataFrame({
    'Year': [2015, 2016, 2017, 2018, 2019, 2020],
    'Actual': [10, 15, 13, 18, 16, 20],
    'Predicted': [11, 14, 12, 17, 15, 19]
})

fig = go.Figure()
fig.add_trace(go.Scatter(x=data['Year'], y=data['Actual'], mode='lines+markers', name='Actual'))
fig.add_trace(go.Scatter(x=data['Year'], y=data['Predicted'], mode='lines+markers', name='Predicted'))
fig.update_layout(title='Actual vs Predicted Trend (Interactive)',
                  xaxis_title='Year', yaxis_title='Value')
fig.show()

import plotly.express as px

# Example with two features
fig = px.scatter(data, x='Actual', y='Predicted', trendline='ols', title='Actual vs Predicted Scatter Plot')
fig.show()

custom_colors = ['#FF5733', '#33FF57', '#3357FF', '#FFC300', '#DA33FF', '#33FFF9']

fig = px.scatter(
    data,
    x='Actual',
    y='Predicted',
    color='Year',
    trendline='ols',
    color_discrete_sequence=custom_colors,
    title='Actual vs Predicted Scatter Plot'
)

fig.update_traces(marker=dict(size=10, line=dict(width=1, color='black')))
fig.show()

import plotly.express as px
# Convert PySpark DataFrame to Pandas
pdf_2030 = pred_2030.select("LocationID", "prediction", "Year").toPandas()

# Custom colors (you can add more if you have more points)
custom_colors = ['#FF5733', '#33FF57', '#3357FF', '#FFC300', '#DA33FF', '#33FFF9']

# Create scatter plot for 2030 predictions
fig = px.scatter(
    pdf_2030,
    x='LocationID',
    y='prediction',
    color='LocationID',  # Each location gets a different color
    color_discrete_sequence=custom_colors,
    title='Predicted Health Metric Values for 2030 by LocationID',
    labels={'prediction': 'Predicted Data Value', 'LocationID': 'Location ID'},
    size='prediction',  # Optional: make bigger markers for higher values
    hover_data=['Year'])
# Customize marker style
fig.update_traces(marker=dict(size=12, line=dict(width=1, color='black')))
fig.update_layout(template='plotly_white')
fig.show()

